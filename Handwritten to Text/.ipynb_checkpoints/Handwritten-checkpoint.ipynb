{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28896f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def resize_image(image, target_size):\n",
    "#     resized_image = cv2.resize(image, target_size)\n",
    "#     return resized_image\n",
    "\n",
    "# def normalize_image(image):\n",
    "#     normalized_image = image.astype(np.float32) / 255.0\n",
    "#     return normalized_image\n",
    "\n",
    "# def remove_noise(image):\n",
    "#     blurred_image = cv2.medianBlur(image, 5)\n",
    "#     return blurred_image\n",
    "\n",
    "# def augment_data(image):\n",
    "#     # Example of data augmentation: rotation\n",
    "#     angle = np.random.randint(-10, 10)  # Random rotation angle between -10 and 10 degrees\n",
    "#     rows, cols = image.shape[:2]\n",
    "#     rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "#     augmented_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))\n",
    "#     return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96034bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # Resize image\n",
    "# target_size = (500, 500)\n",
    "# resized_image = resize_image(image, target_size)\n",
    "\n",
    "# # Normalize image\n",
    "# normalized_image = normalize_image(resized_image)\n",
    "\n",
    "# # Remove noise\n",
    "# denoised_image = remove_noise(normalized_image)\n",
    "\n",
    "# # Augment data\n",
    "# augmented_image = augment_data(denoised_image)\n",
    "\n",
    "# # Display the preprocessed images\n",
    "# cv2.imshow(\"Original Image\", image)\n",
    "# cv2.imshow(\"Resized Image\", resized_image)\n",
    "# cv2.imshow(\"Normalized Image\", normalized_image)\n",
    "# cv2.imshow(\"Denoised Image\", denoised_image)\n",
    "# cv2.imshow(\"Augmented Image\", augmented_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a98a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# # Define the CNN-RNN model\n",
    "# def build_model(input_shape, num_classes):\n",
    "#     # CNN layers\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Flatten())\n",
    "    \n",
    "#     # RNN layers\n",
    "#     model.add(layers.Reshape((10, -1)))  # Reshape CNN output to match RNN input shape\n",
    "#     model.add(layers.GRU(128, return_sequences=True))\n",
    "#     model.add(layers.GRU(128))\n",
    "    \n",
    "#     # Output layer\n",
    "#     model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Example usage\n",
    "# input_shape = (28, 28, 1)  # Adjust according to your input image size\n",
    "# num_classes = 26  # Number of classes for alphabet recognition (adjust based on your specific task)\n",
    "\n",
    "# # Build the model\n",
    "# model = build_model(input_shape, num_classes)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load your preprocessed dataset (input images and corresponding labels)\n",
    "# X = # Preprocessed input images\n",
    "# y = # Corresponding digital text labels\n",
    "\n",
    "# # Split the dataset into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the model architecture\n",
    "# input_shape = (28, 28, 1)  # Adjust according to your input image size\n",
    "# num_classes = 26  # Number of classes for alphabet recognition (adjust based on your specific task)\n",
    "# model = build_model(input_shape, num_classes)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# batch_size = 32\n",
    "# epochs = 10\n",
    "\n",
    "# model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model on the validation set\n",
    "# loss, accuracy = model.evaluate(X_val, y_val)\n",
    "# print(\"Validation loss:\", loss)\n",
    "# print(\"Validation accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93c61450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make\n",
      "Q\n",
      "handwriting\n",
      "fontl\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# Create an EasyOCR reader with English language support\n",
    "reader = easyocr.Reader([\"en\"], gpu=False)\n",
    "\n",
    "# Path to the handwritten image\n",
    "image_path = \"/home/gimhara/Desktop/Programings/Machine Learning/My/07-Handwritten/Handwritten_texts/input_001.jpeg\"\n",
    "\n",
    "# Perform text recognition on the image\n",
    "bounds = reader.readtext(image_path)\n",
    "\n",
    "# Extract the text from the recognized bounds\n",
    "recognized_text = [bound[1] for bound in bounds]\n",
    "\n",
    "# Path to the output text file\n",
    "output_file = \"/home/gimhara/Desktop/Programings/Machine Learning/My/07-Handwritten/saved/output_text.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, \"w\") as file:\n",
    "    # Iterate over the recognized text\n",
    "    for text in recognized_text:\n",
    "        print(text)  # Print the text to the console\n",
    "        file.write(text + \"\\n\")  # Write the text to the file, followed by a new line\n",
    "\n",
    "# The 'with' statement automatically closes the file after writing\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107e6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
